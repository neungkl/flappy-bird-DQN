{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 300\n",
    "MAX_STEPS = 500\n",
    "GAMMA = 0.99\n",
    "\n",
    "EXPLORE_INIT = 1.0\n",
    "EXPLORE_FINAL = 0.01\n",
    "EXPLORE_DECAY = 0.01\n",
    "\n",
    "MEMORY_SIZE = 10000\n",
    "MEMORT_START_SIZE = 5000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "ACTION_SIZE = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Q_model(learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, activation='relu', input_shape=(STATE_SIZE,)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(ACTION_SIZE, activation='linear'))\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(\n",
    "            np.arange(len(self.buffer)),\n",
    "            size=batch_size,\n",
    "            replace=False\n",
    "        )\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoard:\n",
    "\n",
    "    def __init__(self,\n",
    "#                  model,\n",
    "                 log_dir='./logs',\n",
    "                 write_graph=False):\n",
    "        \n",
    "        global tf, projector\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        self.write_graph = write_graph\n",
    "        \n",
    "        self.sess = K.get_session()\n",
    "\n",
    "        if self.write_graph:\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir,\n",
    "                                                self.sess.graph)\n",
    "        else:\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "            \n",
    "    def save(self, steps, logs):\n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(\n",
    "                summary,\n",
    "                steps\n",
    "            )\n",
    "            \n",
    "        self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_Q_model()\n",
    "memory = Memory(max_size = MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Episode 1, Total Reward 10.0, Explore Rate 1.0\n",
      "Episode 2, Total Reward 12.0, Explore Rate 0.9901493354116764\n",
      "Episode 3, Total Reward 26.0, Explore Rate 0.9803966865736877\n",
      "Episode 4, Total Reward 17.0, Explore Rate 0.970741078213023\n",
      "Episode 5, Total Reward 16.0, Explore Rate 0.9611815447608\n",
      "Episode 6, Total Reward 39.0, Explore Rate 0.9517171302557069\n",
      "Episode 7, Total Reward 18.0, Explore Rate 0.9423468882484062\n",
      "Episode 8, Total Reward 38.0, Explore Rate 0.9330698817068888\n",
      "Episode 9, Total Reward 26.0, Explore Rate 0.9238851829227694\n",
      "Episode 10, Total Reward 22.0, Explore Rate 0.9147918734185159\n",
      "Episode 11, Total Reward 15.0, Explore Rate 0.9057890438555999\n",
      "Episode 12, Total Reward 13.0, Explore Rate 0.896875793943563\n",
      "Episode 13, Total Reward 12.0, Explore Rate 0.888051232349986\n",
      "Episode 14, Total Reward 37.0, Explore Rate 0.8793144766113556\n",
      "Episode 15, Total Reward 15.0, Explore Rate 0.8706646530448178\n",
      "Episode 16, Total Reward 12.0, Explore Rate 0.8621008966608072\n",
      "Episode 17, Total Reward 13.0, Explore Rate 0.8536223510765493\n",
      "Episode 18, Total Reward 27.0, Explore Rate 0.8452281684304199\n",
      "Episode 19, Total Reward 15.0, Explore Rate 0.8369175092971592\n",
      "Episode 20, Total Reward 48.0, Explore Rate 0.8286895426039287\n",
      "Episode 21, Total Reward 38.0, Explore Rate 0.820543445547202\n",
      "Episode 22, Total Reward 10.0, Explore Rate 0.8124784035104852\n",
      "Episode 23, Total Reward 17.0, Explore Rate 0.8044936099828537\n",
      "Episode 24, Total Reward 21.0, Explore Rate 0.7965882664783007\n",
      "Episode 25, Total Reward 58.0, Explore Rate 0.7887615824558879\n",
      "Episode 26, Total Reward 108.0, Explore Rate 0.7810127752406908\n",
      "Episode 27, Total Reward 39.0, Explore Rate 0.7733410699455306\n",
      "Episode 28, Total Reward 16.0, Explore Rate 0.7657456993934846\n",
      "Episode 29, Total Reward 24.0, Explore Rate 0.7582259040411682\n",
      "Episode 30, Total Reward 15.0, Explore Rate 0.7507809319027796\n",
      "Episode 31, Total Reward 11.0, Explore Rate 0.7434100384749007\n",
      "Episode 32, Total Reward 12.0, Explore Rate 0.7361124866620463\n",
      "Episode 33, Total Reward 25.0, Explore Rate 0.7288875467029541\n",
      "Episode 34, Total Reward 12.0, Explore Rate 0.7217344960976069\n",
      "Episode 35, Total Reward 67.0, Explore Rate 0.7146526195349836\n",
      "Episode 36, Total Reward 108.0, Explore Rate 0.7076412088215263\n",
      "Episode 37, Total Reward 11.0, Explore Rate 0.7006995628103208\n",
      "Episode 38, Total Reward 36.0, Explore Rate 0.6938269873309811\n",
      "Episode 39, Total Reward 10.0, Explore Rate 0.6870227951202322\n",
      "Episode 40, Total Reward 57.0, Explore Rate 0.680286305753183\n",
      "Episode 41, Total Reward 17.0, Explore Rate 0.6736168455752829\n",
      "Episode 42, Total Reward 48.0, Explore Rate 0.6670137476349562\n",
      "Episode 43, Total Reward 15.0, Explore Rate 0.6604763516169062\n",
      "Episode 44, Total Reward 17.0, Explore Rate 0.6540040037760834\n",
      "Episode 45, Total Reward 25.0, Explore Rate 0.64759605687231\n",
      "Episode 46, Total Reward 44.0, Explore Rate 0.6412518701055556\n",
      "Episode 47, Total Reward 17.0, Explore Rate 0.6349708090518567\n",
      "Episode 48, Total Reward 88.0, Explore Rate 0.6287522455998737\n",
      "Episode 49, Total Reward 24.0, Explore Rate 0.6225955578880794\n",
      "Episode 50, Total Reward 182.0, Explore Rate 0.616500130242572\n",
      "Episode 51, Total Reward 32.0, Explore Rate 0.6104653531155071\n",
      "Episode 52, Total Reward 43.0, Explore Rate 0.6044906230241432\n",
      "Episode 53, Total Reward 16.0, Explore Rate 0.5985753424904925\n",
      "Episode 54, Total Reward 74.0, Explore Rate 0.5927189199815717\n",
      "Episode 55, Total Reward 91.0, Explore Rate 0.5869207698502498\n",
      "Episode 56, Total Reward 110.0, Explore Rate 0.5811803122766818\n",
      "Episode 57, Total Reward 14.0, Explore Rate 0.5754969732103268\n",
      "Episode 58, Total Reward 52.0, Explore Rate 0.5698701843125418\n",
      "Episode 59, Total Reward 60.0, Explore Rate 0.564299382899748\n",
      "Episode 60, Total Reward 80.0, Explore Rate 0.558784011887162\n",
      "Episode 61, Total Reward 35.0, Explore Rate 0.5533235197330861\n",
      "Episode 62, Total Reward 108.0, Explore Rate 0.5479173603837548\n",
      "Episode 63, Total Reward 116.0, Explore Rate 0.5425649932187278\n",
      "Episode 64, Total Reward 101.0, Explore Rate 0.5372658829968282\n",
      "Episode 65, Total Reward 149.0, Explore Rate 0.5320194998026181\n",
      "Episode 66, Total Reward 73.0, Explore Rate 0.5268253189934059\n",
      "Episode 67, Total Reward 58.0, Explore Rate 0.5216828211467822\n",
      "Episode 68, Total Reward 64.0, Explore Rate 0.516591492008677\n",
      "Episode 69, Total Reward 167.0, Explore Rate 0.5115508224419336\n",
      "Episode 70, Total Reward 30.0, Explore Rate 0.5065603083753949\n",
      "Episode 71, Total Reward 159.0, Explore Rate 0.5016194507534953\n",
      "Episode 72, Total Reward 111.0, Explore Rate 0.49672775548635545\n",
      "Episode 73, Total Reward 54.0, Explore Rate 0.491884733400372\n",
      "Episode 74, Total Reward 83.0, Explore Rate 0.48708990018930043\n",
      "Episode 75, Total Reward 63.0, Explore Rate 0.48234277636582407\n"
     ]
    }
   ],
   "source": [
    "explore_rate = EXPLORE_INIT\n",
    "step = 0\n",
    "ep = 0\n",
    "begin_train = False\n",
    "\n",
    "tensorboard = TensorBoard()\n",
    "print(\"start\")\n",
    "\n",
    "for run_ep in range(0, EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    if begin_train:\n",
    "        ep += 1\n",
    "        \n",
    "    explore_rate = EXPLORE_FINAL + (EXPLORE_INIT - EXPLORE_FINAL) * np.exp(-EXPLORE_DECAY * ep)\n",
    "    \n",
    "    is_print = False\n",
    "    \n",
    "    for time_t in range(MAX_STEPS):\n",
    "        if begin_train and np.random.rand() > explore_rate:\n",
    "            action = np.argmax(model.predict(state)[0])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        \n",
    "#         env.render()\n",
    "\n",
    "        memory.add((state, action, reward, done, next_state))\n",
    "        state = next_state\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if memory.size() >= MEMORT_START_SIZE:\n",
    "            begin_train = True\n",
    "            minibatch = memory.sample(BATCH_SIZE)\n",
    "            \n",
    "            inputs = np.zeros((BATCH_SIZE, STATE_SIZE))\n",
    "            targets = np.zeros((BATCH_SIZE, ACTION_SIZE))\n",
    "            \n",
    "            for i, (state_b, action_b, reward_b, done_b, next_state_b) in enumerate(minibatch):\n",
    "                inputs[i] = state_b[0]\n",
    "                \n",
    "                target = reward_b\n",
    "                if not done_b:\n",
    "                    target = reward_b + GAMMA * np.amax(model.predict(next_state_b))\n",
    "                \n",
    "                targets[i] = model.predict(state_b)\n",
    "                targets[i][action_b] = target\n",
    "        \n",
    "            history = model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "            step += 1\n",
    "            tensorboard.save(step, {'loss': history.history['loss'][-1]})\n",
    "        \n",
    "        if done:\n",
    "            if begin_train:\n",
    "                tensorboard.save(\n",
    "                    ep + 1,\n",
    "                    {'total_reward': total_reward, 'explore_rate': explore_rate}\n",
    "                )\n",
    "                log_message = \"Episode {}, Total Reward {}, Explore Rate {}\".format(ep + 1, total_reward, explore_rate)\n",
    "                print(log_message)\n",
    "            break\n",
    "            \n",
    "    if ep % 20 == 0:\n",
    "        model.save('model.h5')\n",
    "\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
